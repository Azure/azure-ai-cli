# - area: ai init
#   tags: [before]
#   tests:
#   - name: ai init openai
#     command: ai init openai
#     arguments:
#       subscription: e72e5254-f265-4e95-9bd2-9ee8e7329051
#       name: robch-oai-eastus2
#       chat-deployment-name: gpt-4-32k-0613
#       embeddings-deployment-name: text-embedding-ada-002-2
#       evaluation-deployment-name: gpt-4-32k-0613
#       interactive: false

- name: check cli is installed
  command: ai
  expect: |
    ^AI - Azure AI CLI, Version 1\.0\..*\r?$\n
    ^Copyright \(c\) 2024 Microsoft Corporation\. All Rights Reserved\.\r?$\n
  tag: skip

- name: check az is installed and we're logged in
  bash: |
    az account show
  expect: |
    "id": "[-a-z0-9-]{36}",
  tag: skip

- name: check ai config for chat
  bash: |
    ai config chat @key
    ai config chat @endpoint
    ai config chat @deployment
    ai config chat @model
  expect: |
    (?# ---------- CHAT KEY)
    chat.key \(found at '.*/.ai/data'\)
    [a-z0-9]{32}

    (?# ---------- CHAT ENDPOINT)
    chat.endpoint \(found at '.*/.ai/data'\)
    https://.*openai.azure.com/

    (?# ---------- CHAT DEPLOYMENT)
    chat.deployment \(found at '.*/.ai/data'\)

    (?# ---------- CHAT MODEL)
    chat.model \(found at '.*/.ai/data'\)
    (gpt4|gpt-35)
  tag: skip

- name: test ai search index update
  command: ai search index update --files "./data/3-product-info/*.md" --index-name "product-info"
  expect: |
    Updating search index 'product-info' ...
    Updating search index 'product-info' ... Done!
    search.index.name \(saved at .*/.ai/data\)
      product-info
  tag: skip

- name: check search index name
  command: ai config @search.index.name
  expect: product-info
  tag: skip

- name: test ai dev new .env
  command: ai dev new .env
  expect: |
    dev.new.env \(saved at .*/.ai/data\)
      AZURE_AI_SEARCH_ENDPOINT = 
      AZURE_AI_SEARCH_INDEX_NAME = 
      AZURE_AI_SEARCH_KEY = 
      AZURE_AI_SPEECH_ENDPOINT = 
      AZURE_AI_SPEECH_KEY = 
      AZURE_AI_SPEECH_REGION = 
      AZURE_COGNITIVE_SEARCH_KEY = 
      AZURE_COGNITIVE_SEARCH_TARGET = 
      AZURE_OPENAI_API_VERSION = 
      AZURE_OPENAI_CHAT_DEPLOYMENT = 
      AZURE_OPENAI_CHAT_MODEL = 
      AZURE_OPENAI_EMBEDDING_DEPLOYMENT = 
      AZURE_OPENAI_EMBEDDING_MODEL = 
      AZURE_OPENAI_ENDPOINT = 
      AZURE_OPENAI_EVALUATION_DEPLOYMENT = 
      AZURE_OPENAI_EVALUATION_MODEL = 
      AZURE_OPENAI_KEY = 
      OPENAI_API_BASE = 
      OPENAI_API_KEY = 
      OPENAI_API_TYPE = 
      OPENAI_API_VERSION = 
      OPENAI_ENDPOINT = 
  tag: skip

- name: test python src/run.py --question "which tent is the most waterproof?"
  command: ai dev shell
  arguments:
    run: python src/run.py --question "which tent is the most waterproof?"
  tag: skip

- name: test python src/run.py --question "which tent is the most waterproof?" --implementation promptflow
  command: ai dev shell
  arguments:
    run: python src/run.py --question "which tent is the most waterproof?" --implementation promptflow
  tag: skip # doesn't work currently

- name: test ai chat --interactive
  command: ai chat --interactive
  input: |
    which tent is the most waterproof?
  tag: skip

- name: test ai chat --interactive --function src/copilot_aisdk/chat:chat_completion
  command: ai chat --interactive --function src/copilot_aisdk/chat:chat_completion
  input: |
    which tent is the most waterproof?
  tag: skip

- name: test python src/run.py --evaluate --implementation aisdk
  command: ai dev shell
  arguments:
    run: python src/run.py --evaluate --implementation aisdk
  expect: |
    {
    ('(gpt_coherence|gpt_groundedness|gpt_relevance)': .*){3}
    }
  tag: skip

- name: test ai chat evaluate --input-data src/tests/evaluation_dataset.jsonl
  command: ai chat evaluate --input-data src/tests/evaluation_dataset.jsonl
  expect: |
    Evaluating chats ...
    {
    ('(gpt_coherence|gpt_groundedness|gpt_relevance)': .*){3}
    }
  tag: skip

- name: test ai chat evaluate --input-data src/tests/evaluation_dataset.jsonl --function src/copilot_aisdk/chat:chat_completion
  command: ai chat evaluate --input-data src/tests/evaluation_dataset.jsonl --function src/copilot_aisdk/chat:chat_completion
  expect: |
    Evaluating chats ...
    {
    ('(gpt_coherence|gpt_groundedness|gpt_relevance)': .*){3}
    }
  tag: skip

- name: test pytest
  command: ai dev shell
  arguments:
    run: pytest
  tag: skip # doesn't work currently

- name: test python src/run.py --deploy
  command: ai dev shell
  arguments:
    run: python src/run.py --deploy
  tag: skip # doesn't work currently (returns exit code 1)
